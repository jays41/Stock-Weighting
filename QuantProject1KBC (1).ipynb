{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dcb6271-39b8-48a8-afc2-cffe626bf499",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cvxpy as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d84a1452-ee4a-4906-ad83-208708de6a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK 1:INPUTS\n",
    "np.random.seed(20) #seeded to get same results each time, can change when needed\n",
    "\n",
    "stock_prices = pd.read_csv(\"stock_prices.csv\", parse_dates = [\"date\"])\n",
    "#clarification, parse_dates added for DateTime casting\n",
    "sxp = pd.read_csv(\"s&p_data.csv\", parse_dates = [\"Date\"])\n",
    "end_date = sxp[\"Date\"].max() #Recent datapoints\n",
    "start_date = end_date - pd.Timedelta(days=365) #last year\n",
    "filtered_data = stock_prices[(stock_prices[\"date\"] >= start_date) & (stock_prices[\"date\"] <= end_date)].copy()\n",
    "\n",
    "\n",
    "days_traded_stock = filtered_data.groupby(\"ticker\")[\"date\"].nunique()\n",
    "qualified_tickers = days_traded_stock[days_traded_stock >= 150].index #List of STOCK TICKERS\n",
    "#150 days chosen but can be adjusted if more qualified stock tickers are needed\n",
    "\n",
    "randomised_tickers = np.random.choice(qualified_tickers, size = min(20, len(qualified_tickers)))\n",
    "#can implement random.normal if needed with an array of tickers but not necessary... I think???\n",
    "\n",
    "latest_data = filtered_data.sort_values(\"date\").groupby(\"ticker\").tail(1).set_index(\"ticker\") #Oldest to newest with last row(recent closing price)\n",
    "latest_price = latest_data[\"close\"]\n",
    "sectors = latest_data[\"sector\"]\n",
    "\n",
    "#fake mock uplifts\n",
    "mock_price_increase_values = np.random.uniform(0.20, 0.40, len(randomised_tickers)) #can change with actual input values when received\n",
    "mock_price_increase = pd.Series(mock_price_increase_values, index= randomised_tickers)\n",
    "\n",
    "#random months till it will hit target\n",
    "target_horizon = pd.Series(np.random.choice([3,6,9,12], size = len(randomised_tickers)), index = randomised_tickers) #can change depending on what target_horizon is desired\n",
    "target_price = latest_price.reindex(randomised_tickers) * (1+ mock_price_increase) #reindex due to array length mismatch!!\n",
    "\n",
    "#Fake betas\n",
    "betas = pd.Series(np.random.uniform(0.7, 1.3, size = len(randomised_tickers)), index = randomised_tickers) #betas are randomised but can be set to 1 if needed.....\n",
    "#chosen uniform, any better dists???\n",
    "\n",
    "#Making the DataFrame from inputs\n",
    "\n",
    "inputs_df = pd.DataFrame({\n",
    "    \"ticker_name\": randomised_tickers,\n",
    "    \"latest_price\": latest_price.reindex(randomised_tickers).values,\n",
    "    \"target_price\": target_price.values,\n",
    "    \"target_horizon\": target_horizon.values,\n",
    "    \"beta\": betas.values,\n",
    "    \"sector\": sectors.reindex(randomised_tickers).values\n",
    "\n",
    "})\n",
    "\n",
    "#made use of .reindex() to fix the mismatch of array lengths\n",
    "\n",
    "#Task 2 Expected Returns\n",
    "\n",
    "\n",
    "\n",
    "inputs_df[\"expected_return\"] = (((inputs_df[\"target_price\"]/ inputs_df[\"latest_price\"])) ** (12/inputs_df[\"target_horizon\"]) -1) #Expected return formula \n",
    "\n",
    "\n",
    "#Task 3 Covariance variance Matrix\n",
    "\n",
    "price_set = filtered_data[filtered_data[\"ticker\"].isin(randomised_tickers)].copy() # selecting the prices from the price dataset with respect to the chosen qualified tickers\n",
    "price_set = price_set.sort_values([\"ticker\", \"date\"]) #reformatting columns\n",
    "price_set[\"daily_return\"] = price_set.groupby(\"ticker\")[\"close\"].pct_change() #percentage change of each ticker within each date\n",
    "new_returns = price_set.pivot(index = \"date\", columns = \"ticker\", values = \"daily_return\") #just for helping with visualisation- ticker vs date col, row\n",
    "covariance_matrix = new_returns.cov() #ticker vs ticker matrix\n",
    "\n",
    "\n",
    "\n",
    "#Task 4 -Put returns + beta here, conditions: low risk, fully-invested, in line with S&P benchmark\n",
    "\n",
    "\n",
    "returns = inputs_df[\"expected_return\"].values\n",
    "betas = inputs_df[\"beta\"].values\n",
    "new_covariance_matrix = covariance_matrix.reindex(index= randomised_tickers, columns = randomised_tickers).values #incase tickers order is switched and in specific place(got an error without this)\n",
    "n = len(randomised_tickers)\n",
    "weights_vector = cp.Variable(n) #vector of [w1,w2.....wn]\n",
    "target_task = cp.Minimize(cp.quad_form(weights_vector,new_covariance_matrix)) #minimize the portfolio variance\n",
    "#tried cp.sum_squares() but non-canonical!\n",
    "conditions = [cp.sum(weights_vector) == 1, weights_vector >= 0, betas @ weights_vector == 1] #waiting for caps??? applied the other conditions\n",
    "problem = cp.Problem(target_task, conditions)\n",
    "problem.solve()\n",
    "#solution is found in the weights_vector where correct weightings of each stock ticker are found\n",
    "\n",
    "#Task 5\n",
    "\n",
    "inputs_df[\"optimal_weights\"] = np.array(weights_vector.value)\n",
    "expected_portfolio_return = returns @ np.array(weights_vector.value) #dot product of return and weights \n",
    "\n",
    "\n",
    "portfolio_volatility = np.sqrt(np.array(weights_vector.value).T @ new_covariance_matrix @ np.array(weights_vector.value))\n",
    "portfolio_beta = betas @ np.array(weights_vector.value)\n",
    "sharpe_ratio = expected_portfolio_return / portfolio_volatility\n",
    "\n",
    "#print(\"Expected return: {}\".format(expected_portfolio_return))\n",
    "#print(\"Expected volatility: {}\".format(portfolio_volatility))\n",
    "#print(\"Beta: {}\".format(portfolio_beta))\n",
    "#print(\"Sharpe ratio: {}\".format(sharpe_ratio))\n",
    "\n",
    "\n",
    "sector_exposure = inputs_df.groupby(\"sector\")[\"optimal_weights\"].sum().sort_values()\n",
    "#print(sector_exposure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f24a8fa-870d-4d39-a576-25e5d6f19534",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32a3899-3463-47ba-9e27-374d757c94f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
